# git checkout e61808a8d
# pytest pybuda/test/benchmark/benchmark.py -m resnet -c resnet50 -opt 4 --loop_count 128 -mb 64 --arch wormhole_b0

devices:
  arch: wormhole_b0

queues:

  # input
  input_1:                                                                                   {input: HOST, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 392], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: host, host: [0x0]}

  # output
  resnet50.output_add_759:                                                                   {input: add_759, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x3598020]}

  # parameter
  conv1.weight:                                                                              {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x57974a0], [5, 0x41edfe20], [0, 0x419ff820], [1, 0x96436a0]]}
  conv1.weight_fork_clone1919:                                                               {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x418a90a0], [2, 0x5fe3840], [2, 0x419adf00], [3, 0xc488520]]}
  conv1.weight_fork_clone1921:                                                               {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41273a00], [4, 0xbc3e1e0], [4, 0x410e8c60], [5, 0x57951c0]]}
  conv1.weight_fork_clone1923:                                                               {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc486320], [3, 0x41273120], [4, 0xbc3d900], [4, 0x410e8380]]}
  layer1.0.conv1.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [1, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5792fe0]]}
  layer1.0.conv2.weight_fork_clone1327:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 2], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5fe8100]]}
  layer1.0.conv2.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 2], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41a02420]]}
  layer1.0.conv2.weight_fork_clone1325:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 2], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5797d80]]}
  layer1.0.conv3.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x419b04a0]]}
  layer1.0.downsample.0.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9643f80]]}
  layer1.1.conv1.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [4, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5fdbc40]]}
  layer1.1.conv2.weight_fork_clone1374:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 2], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9635f80]]}
  layer1.1.conv2.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 2], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5786640]]}
  layer1.1.conv2.weight_fork_clone1372:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 2], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41268380]]}
  layer1.1.conv3.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5fd6160]]}
  layer1.2.conv1.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [4, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x419eb1a0]]}
  layer1.2.conv2.weight_fork_clone1408:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 2], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x963b740]]}
  layer1.2.conv2.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 2], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x578fb40]]}
  layer1.2.conv2.weight_fork_clone1406:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 2], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc482200]]}
  layer1.2.conv3.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4189e7c0]]}
  layer2.0.conv1.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x419f0c80]]}
  layer2.0.conv2.weight_fork_clone1442:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xbc42180]]}
  layer2.0.conv2.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xbc90b00]]}
  layer2.0.conv2.weight_fork_clone1440:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 1], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x419ed560]]}
  layer2.0.conv3.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9654de0]]}
  layer2.0.downsample.0.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 1], ublock: [4, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xbc7f2e0], [4, 0x410fa540]]}
  layer2.1.conv1.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x419dbd40]]}
  layer2.1.conv2.weight_fork_clone1491:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 4], ublock: [4, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41a22300]]}
  layer2.1.conv2.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 4], ublock: [4, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x602df20]]}
  layer2.1.conv2.weight_fork_clone1489:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [3, 4], ublock: [4, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xbc9dd20]]}
  layer2.1.conv3.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x57fbb80]]}
  layer2.2.conv1.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4127afc0]]}
  layer2.2.conv2.weight_fork_clone1525:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [12, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x42624340]]}
  layer2.2.conv2.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [12, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4547a120]]}
  layer2.2.conv2.weight_fork_clone1523:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [12, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc48c860]]}
  layer2.2.conv3.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x42612b20]]}
  layer2.3.conv1.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [16, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41a058c0]]}
  layer2.3.conv2.weight_fork_clone1559:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [12, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x57ed920]]}
  layer2.3.conv2.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [12, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4263a180]]}
  layer2.3.conv2.weight_fork_clone1557:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [12, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x45488380]]}
  layer2.3.conv3.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xbc6ca20]]}
  layer3.0.conv1.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [4, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x42631560], [2, 0x6007b80], [2, 0x419d20e0], [3, 0xc49ac20]]}
  layer3.0.conv2.weight_fork_clone1593:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [24, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x579db60]]}
  layer3.0.conv2.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [24, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xba696c0]]}
  layer3.0.conv2.weight_fork_clone1591:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [24, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4191ddc0]]}
  layer3.0.conv3.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x40fa60a0]]}
  layer3.0.downsample.0.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [2, 2], ublock: [8, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5dbf580], [1, 0x417281a0]]}
  layer3.1.conv1.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [32, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x55bc100]]}
  layer3.1.conv2.weight_fork_clone1642:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [24, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc1cc960]]}
  layer3.1.conv2.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [24, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5e06fa0]]}
  layer3.1.conv2.weight_fork_clone1640:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [24, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5603b20]]}
  layer3.1.conv3.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x40f84720]]}
  layer3.2.conv1.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [32, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x41747900]]}
  layer3.2.conv2.weight_fork_clone1676:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [24, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41759f40]]}
  layer3.2.conv2.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [24, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x40ff3e00]]}
  layer3.2.conv2.weight_fork_clone1674:                                                      {input: HOST, type: ram, entries: 1, grid_size: [6, 1], t: 1, mblock: [1, 2], ublock: [4, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5d6f900], [1, 0x4170e620], [2, 0x5d680a0], [2, 0x41738180], [3, 0xc1c16a0], [3, 0x40f67c20]]}
  layer3.2.conv3.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 8], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc1adb60], [3, 0x40f540e0], [4, 0xba0e760], [4, 0x40f5a660]]}
  layer3.3.conv1.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 2], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41701cc0], [1, 0x5d5bdc0], [1, 0x416faae0], [2, 0x5d54560]]}
  layer3.3.conv2.weight_fork_clone1710:                                                      {input: HOST, type: ram, entries: 1, grid_size: [6, 1], t: 1, mblock: [1, 1], ublock: [4, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xba05180], [4, 0x40f51080], [5, 0x55ab180], [5, 0x40f51080], [0, 0x416f90a0], [1, 0x5d531a0]]}
  layer3.3.conv2.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [24, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x416f8060]]}
  layer3.3.conv2.weight_fork_clone1708:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [24, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xba232e0]]}
  layer3.3.conv3.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5d79560]]}
  layer3.4.conv1.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [32, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x40f6fb20]]}
  layer3.4.conv2.weight_fork_clone1744:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [24, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x40f70840]]}
  layer3.4.conv2.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [24, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5d70cc0]]}
  layer3.4.conv2.weight_fork_clone1742:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [24, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41717b20]]}
  layer3.4.conv3.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc38ac80]]}
  layer3.5.conv1.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [32, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5f480a0]]}
  layer3.5.conv2.weight_fork_clone1778:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [24, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x411382a0]]}
  layer3.5.conv2.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [24, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41181760]]}
  layer3.5.conv2.weight_fork_clone1776:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [24, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5f13880]]}
  layer3.5.conv3.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x418d1a00]]}
  layer4.0.conv1.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 2], ublock: [32, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xbbeb960], [4, 0x410976c0]]}
  layer4.0.conv2.weight_fork_clone1812:                                                      {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 2], ublock: [4, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41982180], [1, 0x6034f40]]}
  layer4.0.conv2.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 2], ublock: [4, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc416cc0], [3, 0x411fd040]]}
  layer4.0.conv2.weight_fork_clone1810:                                                      {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 2], ublock: [4, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41919160], [1, 0x5fcbf20]]}
  layer4.0.conv3.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5f8e0c0], [2, 0x419525e0], [3, 0xc3d0ca0], [3, 0x411b7020]]}
  layer4.0.downsample.0.weight:                                                              {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x56ee340], [5, 0x410a2e00], [0, 0x41845180], [1, 0x5ef0f00]]}
  layer4.1.conv1.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [32, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x41798ce0], [2, 0x5e479e0], [2, 0x4181cdc0], [3, 0xc290e60]]}
  layer4.1.conv2.weight_fork_clone1861:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [8, 1], ublock: [6, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc25c640], [3, 0x41071260], [4, 0xbadbc40], [4, 0x410084a0]]}
  layer4.1.conv2.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 2], ublock: [4, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5db8720], [2, 0x4178d920]]}
  layer4.1.conv2.weight_fork_clone1859:                                                      {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 2], ublock: [4, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x565d680], [5, 0x4100a7c0]]}
  layer4.1.conv3.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x40fe77a0], [0, 0x417b31c0], [1, 0x5e5e7e0], [1, 0x41774580], [2, 0x5e23a60], [2, 0x417f8c60], [3, 0xc238d40], [3, 0x4104d960]]}
  layer4.2.conv1.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [4, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc213a00], [3, 0x41028620], [4, 0xbaad400], [4, 0x40fd9c60], [5, 0x5638340], [5, 0x40fc4780], [0, 0x417901a0], [1, 0x5e3b7c0]]}
  layer4.2.conv2.weight_fork_clone1895:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [8, 1], ublock: [6, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5f7cf20], [1, 0x4184a760], [2, 0x5edf060], [2, 0x418e8560]]}
  layer4.2.conv2.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [8, 1], ublock: [6, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x418b3d40], [3, 0xc3539e0], [3, 0x4114bf00], [4, 0xbbb68e0]]}
  layer4.2.conv2.weight_fork_clone1893:                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [8, 1], ublock: [6, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc31f1c0], [3, 0x411176e0], [4, 0xbb820c0], [4, 0x41061600]]}
  layer4.2.conv3.weight:                                                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x410f46c0], [4, 0xbb5f0a0], [4, 0x4103e5e0], [5, 0x56cb320], [5, 0x4107fde0], [0, 0x41822160], [1, 0x5ecdee0], [1, 0x41827040]]}
  fc.weight:                                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [64, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x417d9e20], [1, 0x5e85ba0], [1, 0x417ded00], [2, 0x5e8da00], [2, 0x41862de0], [3, 0xc2d6e80], [3, 0x410ae6a0], [4, 0xbb19080]]}
  fc.bias:                                                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x41074ea0]]}

  # constant
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x5602120], [5, 0x40fba160], [0, 0x41759580], [1, 0x5e055a0], [1, 0x4176e1c0], [2, 0x5db5a40], [2, 0x41746f40]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:  {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x418aa600], [2, 0x5fe4da0], [2, 0x419af460], [3, 0xc489a80], [3, 0x41276960], [4, 0xbc41140], [4, 0x410ebbc0]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x418a9980], [2, 0x5fe4120], [2, 0x419ae7e0], [3, 0xc488e00], [3, 0x41275ce0], [4, 0xbc404c0], [4, 0x410eaf40]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:  {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x41274ca0], [4, 0xbc3f480], [4, 0x410e9f00], [5, 0x5796460], [5, 0x41edede0], [0, 0x419fe7e0], [1, 0x9642660]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x412742e0], [4, 0xbc3eac0], [4, 0x410e9540], [5, 0x5795aa0], [5, 0x41ede420], [0, 0x419fde20], [1, 0x9641ca0]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:  {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x41edd3e0], [0, 0x419fcde0], [1, 0x9640c60], [1, 0x418a8060], [2, 0x5fe2800], [2, 0x419acec0], [3, 0xc4874e0]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.sparse_matmul.9.dc.sparse_matmul.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x418a2de0], [2, 0x5fe0260], [2, 0x419aa920], [3, 0xc4856a0], [3, 0x412724a0], [4, 0xbc3cc80], [4, 0x410e7700]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.sparse_matmul.9.dc.sparse_matmul.1.1:  {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x5794180], [5, 0x41edc3a0], [0, 0x419fbda0], [1, 0x963fc20], [1, 0x418a7020], [2, 0x5fe17c0], [2, 0x419abe80]]}
  input_1_add_1:                                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x419ab5a0]]}
  input_1_add_1_fork_clone1120:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5fe0ee0]]}
  lc.input_tensor.max_pool2d_14.dc.sparse_matmul.5.dc.sparse_matmul.1.0:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 39], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x418a3a60]]}
  lc.input_tensor.max_pool2d_14.dc.sparse_matmul.5.dc.sparse_matmul.1.1:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x419fad60], [1, 0x963ebe0]]}
  input_1_add_16_fork_clone1163:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc486c00]]}
  lc.input_tensor.conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0xc48b3a0]]}
  lc.input_tensor.conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x419b4ac0]]}
  lc.input_tensor.conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x42611660]]}
  lc.input_tensor.conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x96485a0]]}
  lc.input_tensor.conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x579b220], [5, 0x45478720]]}
  lc.input_tensor.conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x410ecc00]]}
  input_1_add_30:                                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x412779a0]]}
  input_1_add_30_fork_clone1095:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc48aac0]]}
  input_1_add_44_fork_clone1005:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5fe5de0]]}
  input_1_add_57_fork_clone1010:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41a00100]]}
  input_1_add_72_fork_clone1130:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x419a9760]]}
  lc.input_tensor.conv2d_85.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x5fda780]]}
  lc.input_tensor.conv2d_85.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x4189d780]]}
  lc.input_tensor.conv2d_85.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x419ef7c0]]}
  lc.input_tensor.conv2d_85.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x41173160]]}
  lc.input_tensor.conv2d_85.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x410e4dc0]]}
  lc.input_tensor.conv2d_85.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x410e3d80]]}
  input_1_add_86:                                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc481040]]}
  input_1_add_86_fork_clone1047:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x419a8e80]]}
  input_1_add_100_fork_clone952:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4189b460]]}
  input_1_add_115_fork_clone1083:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x41172880]]}
  lc.input_tensor.conv2d_128.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x5785180]]}
  lc.input_tensor.conv2d_128.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0xbc3a3a0]]}
  lc.input_tensor.conv2d_128.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 15], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x419f98a0]]}
  lc.input_tensor.conv2d_128.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x41175340]]}
  lc.input_tensor.conv2d_128.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0xbc3c420], [4, 0x410e6ea0]]}
  lc.input_tensor.conv2d_128.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x41271460]]}
  input_1_add_129:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x419aa040]]}
  input_1_add_129_fork_clone993:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc481920]]}
  input_1_add_143_fork_clone899:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9639420]]}
  input_1_add_158_fork_clone965:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x411741a0]]}
  lc.input_tensor.conv2d_171.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 70], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x5789ae0]]}
  lc.input_tensor.conv2d_171.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x410e5e60]]}
  lc.input_tensor.conv2d_171.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 67], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x4126b820]]}
  lc.input_tensor.conv2d_171.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x419d10a0]]}
  lc.input_tensor.conv2d_171.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 68], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x412941c0]]}
  lc.input_tensor.conv2d_171.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0xc4a6ba0]]}
  input_1_add_172:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6012980]]}
  input_1_add_172_fork_clone871:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x42648440]]}
  input_1_add_186_fork_clone769:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x454955a0]]}
  lc.input_tensor.conv2d_198.dc.sparse_matmul.9.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 23], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x45499bc0]]}
  lc.input_tensor.conv2d_198.dc.sparse_matmul.9.1:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x57fab40]]}
  input_1_add_199_fork_clone774:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 1], ublock: [1, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc4a4880], [3, 0x41291ea0]]}
  input_1_add_214_fork_clone913:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x60117e0]]}
  lc.input_tensor.conv2d_227.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x9653d40], [1, 0x426473a0]]}
  lc.input_tensor.conv2d_227.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x41a212c0]]}
  lc.input_tensor.conv2d_227.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x41a2f520], [1, 0x9666600]]}
  lc.input_tensor.conv2d_227.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x426639e0]]}
  lc.input_tensor.conv2d_227.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x419fa780]]}
  lc.input_tensor.conv2d_227.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0xc4a7be0]]}
  input_1_add_228:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41299f60]]}
  input_1_add_228_fork_clone815:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4549bb80]]}
  input_1_add_242_fork_clone714:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x4110bd60]]}
  input_1_add_257_fork_clone859:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc499a80]]}
  lc.input_tensor.conv2d_270.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x410f7380]]}
  lc.input_tensor.conv2d_270.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x6006b40]]}
  lc.input_tensor.conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x964dc00]]}
  lc.input_tensor.conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x41a170e0]]}
  lc.input_tensor.conv2d_270.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x41278280]]}
  lc.input_tensor.conv2d_270.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x41279f80]]}
  input_1_add_271:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x419b5b00]]}
  input_1_add_271_fork_clone757:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5feb5a0]]}
  input_1_add_285_fork_clone663:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x96495e0]]}
  input_1_add_300_fork_clone803:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x45478f80]]}
  lc.input_tensor.conv2d_313.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0xbc4f3a0], [4, 0x410edc40], [5, 0x579ba80]]}
  lc.input_tensor.conv2d_313.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0xbc50440], [4, 0x410eece0], [5, 0x579cb20]]}
  lc.input_tensor.conv2d_313.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x41290e00], [4, 0xbc7e240], [4, 0x410f94a0]]}
  lc.input_tensor.conv2d_313.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x60107a0], [2, 0x419dad00], [3, 0xc4a3840]]}
  lc.input_tensor.conv2d_313.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 21], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x9652040]]}
  lc.input_tensor.conv2d_313.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x41a20280]]}
  input_1_add_314:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x57d2380]]}
  input_1_add_314_fork_clone702:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 1], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xbc51480]]}
  input_1_add_328_fork_clone610:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4128c7e0]]}
  input_1_add_343_fork_clone675:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x964fd20]]}
  lc.input_tensor.conv2d_356.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 94], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x41a18120]]}
  lc.input_tensor.conv2d_356.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x45487340]]}
  lc.input_tensor.conv2d_356.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 86], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x410efd20]]}
  lc.input_tensor.conv2d_356.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0xbc3b3e0]]}
  lc.input_tensor.conv2d_356.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 91], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x40fec0c0]]}
  lc.input_tensor.conv2d_356.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0xc209da0]]}
  input_1_add_357:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x40f75200]]}
  input_1_add_357_fork_clone580:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xba5a1a0]]}
  input_1_add_371_fork_clone466:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc201180]]}
  lc.input_tensor.conv2d_383.dc.sparse_matmul.9.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 31], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x41741de0]]}
  lc.input_tensor.conv2d_383.dc.sparse_matmul.9.1:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x5db4a00]]}
  input_1_add_384_fork_clone471:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 2], ublock: [1, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x40fb5b40], [0, 0x41754f60]]}
  input_1_add_399_fork_clone624:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x40f72ee0]]}
  lc.input_tensor.conv2d_412.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0xba57b00]]}
  lc.input_tensor.conv2d_412.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x40fa5060]]}
  lc.input_tensor.conv2d_412.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x417448a0]]}
  lc.input_tensor.conv2d_412.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x41771220]]}
  lc.input_tensor.conv2d_412.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x4178e760]]}
  lc.input_tensor.conv2d_412.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x40fc3740]]}
  input_1_add_413:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x40fca740]]}
  input_1_add_413_fork_clone520:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xba9dee0]]}
  input_1_add_427_fork_clone401:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc20ade0]]}
  input_1_add_442_fork_clone568:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5db6400]]}
  lc.input_tensor.conv2d_455.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x4176eb80]]}
  lc.input_tensor.conv2d_455.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x5e05f60]]}
  lc.input_tensor.conv2d_455.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x40f6e1a0]]}
  lc.input_tensor.conv2d_455.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x5602ae0]]}
  lc.input_tensor.conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x416f90a0]]}
  lc.input_tensor.conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0xba222a0]]}
  input_1_add_456:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xc1bf380], [3, 0x40f65900], [4, 0xba1ff80], [4, 0x40f6be80], [5, 0x55b6a80], [5, 0x40f5e2e0], [0, 0x41715800]]}
  input_1_add_456_fork_clone454:                                                             {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x55b4760], [5, 0x40f5bfc0], [0, 0x417134e0], [1, 0x5d6d5e0], [1, 0x4170c300], [2, 0x5d65d80], [2, 0x41735e60]]}
  input_1_add_470_fork_clone344:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4172d240]]}
  input_1_add_485_fork_clone508:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x40f59ca0]]}
  lc.input_tensor.conv2d_498.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x5d53ba0], [2, 0x4172c880], [3, 0xc1ad1a0], [3, 0x40f53720], [4, 0xba0dda0], [4, 0x40f59ca0], [5, 0x55b3da0]]}
  lc.input_tensor.conv2d_498.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x40f50040], [4, 0xba04140], [4, 0x40f50040], [5, 0x55aa140], [5, 0x40f50040], [0, 0x416f8060], [1, 0x5d52160]]}
  lc.input_tensor.conv2d_498.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x40f51080]]}
  lc.input_tensor.conv2d_498.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0xc1ac160]]}
  lc.input_tensor.conv2d_498.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x5d52160]]}
  lc.input_tensor.conv2d_498.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x416f8060]]}
  input_1_add_499:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5da54e0]]}
  input_1_add_499_fork_clone389:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x41718c80]]}
  input_1_add_513_fork_clone291:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x4174c340]]}
  input_1_add_528_fork_clone442:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x55b9de0]]}
  lc.input_tensor.conv2d_541.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x40f70840]]}
  lc.input_tensor.conv2d_541.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x55b8da0]]}
  lc.input_tensor.conv2d_541.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0xc1ca2c0]]}
  lc.input_tensor.conv2d_541.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x41740da0]]}
  lc.input_tensor.conv2d_541.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x41717240]]}
  lc.input_tensor.conv2d_541.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x5d78520]]}
  input_1_add_542:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x40f60600]]}
  input_1_add_542_fork_clone332:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x41998600]]}
  input_1_add_556_fork_clone240:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x40fbab20]]}
  input_1_add_571_fork_clone377:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4188e4a0]]}
  lc.input_tensor.conv2d_584.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x5fc0c60]]}
  lc.input_tensor.conv2d_584.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x41917a20]]}
  lc.input_tensor.conv2d_584.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 28], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x577c3e0]]}
  lc.input_tensor.conv2d_584.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x577b3a0]]}
  lc.input_tensor.conv2d_584.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 19], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0xc389240]]}
  lc.input_tensor.conv2d_584.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x4191cd80]]}
  input_1_add_585:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4187ef80]]}
  input_1_add_585_fork_clone279:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [7, 2], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5fb1740]]}
  input_1_add_599_fork_clone190:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4112f680]]}
  input_1_add_614_fork_clone252:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41266060], [4, 0xbc38080]]}
  lc.input_tensor.conv2d_627.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 14], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x419a7b20], [3, 0xc47fce0]]}
  lc.input_tensor.conv2d_627.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x4189a420], [2, 0x5fd5120]]}
  lc.input_tensor.conv2d_627.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x57840e0], [5, 0x411717e0]]}
  lc.input_tensor.conv2d_627.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0xbc37040], [4, 0x410e2d40]]}
  lc.input_tensor.conv2d_627.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x411b5f80], [4, 0xbc31980]]}
  lc.input_tensor.conv2d_627.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x418993e0], [2, 0x5fd40e0]]}
  input_1_add_628:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x577fac0], [5, 0x4116d1c0]]}
  input_1_add_628_fork_clone163:                                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xbc32a20], [4, 0x410de720]]}
  input_1_add_642_fork_clone82:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5fc3300], [1, 0x418907c0]]}
  lc.input_tensor.conv2d_654.dc.sparse_matmul.9.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x4116cac0], [0, 0x41918a60]]}
  lc.input_tensor.conv2d_654.dc.sparse_matmul.9.1:                                           {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x410dd6e0], [5, 0x577ea80]]}
  input_1_add_655_fork_clone87:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x410a5a80], [4, 0xbb10460]]}
  input_1_add_670_fork_clone203:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x417d7b00], [1, 0x5e83880]]}
  lc.input_tensor.conv2d_683.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 10], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x410740c0]]}
  lc.input_tensor.conv2d_683.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x56c6f80]]}
  lc.input_tensor.conv2d_683.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x5e47180], [2, 0x4181c560]]}
  lc.input_tensor.conv2d_683.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x5e82840], [1, 0x41797ca0]]}
  lc.input_tensor.conv2d_683.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x417975a0], [2, 0x5e46a80]]}
  lc.input_tensor.conv2d_683.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x417d61e0], [1, 0x5e81800]]}
  input_1_add_684:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x40ffefa0]]}
  input_1_add_684_fork_clone116:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xbad2740]]}
  input_1_add_698_fork_clone47:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x41772260], [2, 0x5e21740], [2, 0x417f6940], [3, 0xc236a20], [3, 0x4104b640], [4, 0xbad0420], [4, 0x40ffcc80], [5, 0x565b360]]}
  input_1_add_713_fork_clone151:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4181bc80], [3, 0xc25bd60], [3, 0x41070980], [4, 0xbadb360], [4, 0x41007bc0], [5, 0x56c66a0], [5, 0x410737e0], [0, 0x417d7220]]}
  lc.input_tensor.conv2d_726.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0xbbeb100], [4, 0x41096e60]]}
  lc.input_tensor.conv2d_726.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0xc388200], [3, 0x41180720]]}
  lc.input_tensor.conv2d_726.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x4112ee20], [0, 0x418d11a0]]}
  lc.input_tensor.conv2d_726.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x41095e20], [5, 0x577a360]]}
  lc.input_tensor.conv2d_726.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x4184a060], [2, 0x5ede960]]}
  lc.input_tensor.conv2d_726.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:            {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x4103ccc0], [5, 0x56c7fc0]]}
  input_1_add_727:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x418ab120]]}
  input_1_add_727_fork_clone70:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [2, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5ed5d40]]}
  input_1_add_741_fork_clone24:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x56c9000], [5, 0x4107dac0], [0, 0x4181fe40], [1, 0x5ecbbc0], [1, 0x41824d20], [2, 0x5ed3a20], [2, 0x418a8e00], [3, 0xc31cea0]]}
  lc.input_tensor.avg_pool2d_755.dc.reduce_avg.2.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x4103dd00]]}

  # epoch_to_epoch
  e2e_conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.matmul.11_0:                                       {input: conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.matmul.11, type: queue, entries: 64, grid_size: [7, 1], t: 2, mblock: [7, 1], ublock: [4, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x407a8020], [0, 0x40f50040], [1, 0x55aa140], [1, 0x40f50040], [2, 0x55aa140], [2, 0x40f50040], [3, 0xba04140]]}
  e2e_conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.matmul.11_0:                                       {input: conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.matmul.11, type: queue, entries: 64, grid_size: [7, 1], t: 2, mblock: [7, 1], ublock: [4, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4e02120], [2, 0x407a8020], [3, 0xb25c120], [3, 0x407a8020], [4, 0xb25c120], [4, 0x407a8020], [5, 0x4e02120]]}
  e2e_conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.matmul.11_0:                                       {input: conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.matmul.11, type: queue, entries: 64, grid_size: [7, 1], t: 2, mblock: [7, 1], ublock: [4, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xaab4100], [4, 0x40000000], [5, 0x465a100], [5, 0x40000000], [0, 0x407a8020], [1, 0x4e02120], [1, 0x407a8020]]}
  e2e_conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.matmul.11_0:                                       {input: conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.matmul.11, type: queue, entries: 64, grid_size: [7, 1], t: 2, mblock: [7, 1], ublock: [4, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x40000000], [1, 0x465a100], [1, 0x40000000], [2, 0x465a100], [2, 0x40000000], [3, 0xaab4100], [3, 0x40000000]]}
  e2e_conv2d_241.dc.matmul.8_0:                                                              {input: conv2d_241.dc.matmul.8, type: queue, entries: 64, grid_size: [1, 1], t: 25, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x580d3a0]]}
  e2e_buffer_0_add_211_add_254_0:                                                            {input: buffer_0_add_211_add_254, type: queue, entries: 64, grid_size: [1, 1], t: 25, mblock: [1, 4], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x41110380]]}
  e2e_conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2_0:                    {input: conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [21, 1], ublock: [1, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41a305c0]]}
  e2e_conv2d_455.dc.conv2d.1.dc.matmul.11_0:                                                 {input: conv2d_455.dc.conv2d.1.dc.matmul.11, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [7, 1], ublock: [1, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x42664a20]]}
  e2e_conv2d_455.dc.conv2d.5.dc.matmul.11_0:                                                 {input: conv2d_455.dc.conv2d.5.dc.matmul.11, type: queue, entries: 64, grid_size: [1, 1], t: 1, mblock: [7, 1], ublock: [1, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x96676a0]]}
  e2e_buffer_0_add_439_buffer_0_add_439_buffer_0_add_439_add_482_0:                          {input: buffer_0_add_439_buffer_0_add_439_buffer_0_add_439_add_482, type: queue, entries: 64, grid_size: [1, 1], t: 7, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x454b7120]]}
  e2e_conv2d_484.dc.matmul.8_0:                                                              {input: conv2d_484.dc.matmul.8, type: queue, entries: 64, grid_size: [7, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41341520], [4, 0xbd36f60], [4, 0x42cf43c0], [5, 0x73f13e0], [5, 0x46637160], [0, 0x427dc600], [1, 0x9c6b6e0]]}
  e2e_conv2d_498.dc.conv2d.5.dc.matmul.11_0:                                                 {input: conv2d_498.dc.conv2d.5.dc.matmul.11, type: queue, entries: 64, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x603b140], [2, 0x419fc480], [3, 0xc4a8c20], [3, 0x412b5500], [4, 0xbcaaf40], [4, 0x42c683a0], [5, 0x73653c0]]}
  e2e_buffer_0_add_482_buffer_0_add_482_buffer_0_add_482_buffer_0_add_482_add_525_0:         {input: buffer_0_add_482_buffer_0_add_482_buffer_0_add_482_buffer_0_add_482_add_525, type: queue, entries: 64, grid_size: [7, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x46407140], [0, 0x425ac5e0], [1, 0x9a3b6c0], [1, 0x42a38a40], [2, 0x60c7160], [2, 0x41a884a0], [3, 0xc534c40]]}
  e2e_conv2d_669.dc.matmul.8_0:                                                              {input: conv2d_669.dc.matmul.8, type: queue, entries: 64, grid_size: [2, 2], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x41cb84c0], [3, 0xc764c60], [3, 0x413cd540], [4, 0xbdc2f80]]}
  e2e_conv2d_683.dc.conv2d.1.dc.matmul.11_0:                                                 {input: conv2d_683.dc.conv2d.1.dc.matmul.11, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [1, 2], ublock: [1, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x466c3180], [0, 0x42868620]]}
  e2e_conv2d_683.dc.conv2d.3.dc.matmul.11_0:                                                 {input: conv2d_683.dc.conv2d.3.dc.matmul.11, type: queue, entries: 64, grid_size: [2, 1], t: 1, mblock: [1, 2], ublock: [1, 8], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x42d803e0], [5, 0x747d400]]}
  e2e_add_667_0:                                                                             {input: add_667, type: queue, entries: 64, grid_size: [2, 1], t: 2, mblock: [1, 8], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x42c68a60], [2, 0x62f7180]]}
  e2e_avg_pool2d_755.dc.reduce_avg.2.lc1_0:                                                  {input: avg_pool2d_755.dc.reduce_avg.2.lc1, type: queue, entries: 64, grid_size: [1, 1], t: 8, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9cf7700]]}

  # buffering
  buf_max_pool2d_14.dc.reduce_max.6_0:                                                       {input: max_pool2d_14.dc.reduce_max.6, type: queue, entries: 64, grid_size: [1, 1], t: 14, mblock: [7, 1], ublock: [1, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x41176380]]}
  insert_queue0:                                                                             {input: max_pool2d_14.dc.reduce_max.6, type: queue, entries: 64, grid_size: [1, 1], t: 14, mblock: [7, 1], ublock: [1, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x418ab640]]}
  buf_add_69_0:                                                                              {input: add_69, type: queue, entries: 64, grid_size: [1, 1], t: 2, mblock: [49, 2], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x41ee0700]]}
  insert_queue1:                                                                             {input: add_69, type: queue, entries: 64, grid_size: [1, 1], t: 2, mblock: [49, 2], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x609df60]]}

graphs:
  fwd_0_0_temporal_epoch_0:
    target_device: 0
    input_count: 64
    buffer_0_input_1_conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: nop, grid_loc: [0, 0], grid_size: [7, 1], inputs: [input_1],
         t: 2, mblock: [14, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vslice: 2]}
    conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 1], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, buffer_0_input_1_conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 2, mblock: [14, 1], ublock: [8, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 2],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 5, u_kt: 196}}
    conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 2], grid_size: [7, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, conv1.weight],
         t: 2, mblock: [7, 1], ublock: [4, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 4, hstack: 4],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 3], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, buffer_0_input_1_conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 2, mblock: [14, 1], ublock: [8, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 2],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 5, u_kt: 196}}
    conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 4], grid_size: [7, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, conv1.weight_fork_clone1919],
         t: 2, mblock: [7, 1], ublock: [4, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 4, hstack: 4],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, buffer_0_input_1_conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 2, mblock: [14, 1], ublock: [8, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 2],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 5, u_kt: 196}}
    conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 6], grid_size: [7, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, conv1.weight_fork_clone1921],
         t: 2, mblock: [7, 1], ublock: [4, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 4, hstack: 4],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}
    conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 7], grid_size: [7, 1], inputs: [lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.sparse_matmul.9.dc.sparse_matmul.1.0, buffer_0_input_1_conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 2, mblock: [14, 1], ublock: [8, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 2],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 5, u_kt: 196}}
    conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.matmul.11: {type: matmul, grid_loc: [7, 0], grid_size: [7, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, conv1.weight_fork_clone1923], grid_transpose: true,
         t: 2, mblock: [7, 1], ublock: [4, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 4, hstack: 4],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 1}}

  fwd_0_1_temporal_epoch_1:
    target_device: 0
    input_count: 64
    _fused_op_0: {type: fused_op, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.matmul.11_0, e2e_conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.matmul.11_0, e2e_conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.matmul.11_0, e2e_conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.matmul.11_0, input_1_add_1, input_1_add_1_fork_clone1120],
         t: 2, mblock: [98, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_5_tms: [broadcast: {r: 392}, vslice: 2], input_4_tms: [broadcast: {r: 392}, vslice: 2],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_5: 4, input_4: 4}}}
    max_pool2d_14.dc.sparse_matmul.5.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 1], grid_size: [1, 2], inputs: [lc.input_tensor.max_pool2d_14.dc.sparse_matmul.5.dc.sparse_matmul.1.0, _fused_op_0, lc.input_tensor.max_pool2d_14.dc.sparse_matmul.5.dc.sparse_matmul.1.1],
         t: 14, mblock: [9, 1], ublock: [7, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 2], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 14, num_index_tiles: 2, num_sparse_tiles: 39, sparse_tile_ptr_bits: 7, u_kt: 28}}
    max_pool2d_14.dc.reduce_max.6: {type: reduce, grid_loc: [0, 3], grid_size: [1, 1], inputs: [max_pool2d_14.dc.sparse_matmul.5.dc.sparse_matmul.1.lc2],
         t: 14, mblock: [7, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 9],
         attributes: {dim: z, type: max, z: 9}}
    conv2d_15.dc.matmul.8: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [buf_max_pool2d_14.dc.reduce_max.6_0, layer1.0.conv1.weight, input_1_add_16_fork_clone1163],
         t: 2, mblock: [49, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 98}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vstack: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, m_k: 2, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 1}}
    conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [1, 2], inputs: [lc.input_tensor.conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_15.dc.matmul.8, lc.input_tensor.conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [42, 1], ublock: [7, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 2], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 7, u_kt: 49}}
    conv2d_29.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer1.0.conv2.weight_fork_clone1327],
         t: 1, mblock: [14, 2], ublock: [7, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 1}}
    conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 6], grid_size: [1, 2], inputs: [lc.input_tensor.conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_15.dc.matmul.8, lc.input_tensor.conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [42, 1], ublock: [7, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 2], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 7, u_kt: 49}}
    conv2d_29.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer1.0.conv2.weight],
         t: 1, mblock: [14, 2], ublock: [7, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 1}}
    conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 1], grid_size: [1, 2], inputs: [lc.input_tensor.conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_15.dc.matmul.8, lc.input_tensor.conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [42, 1], ublock: [7, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 2], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 12, sparse_tile_ptr_bits: 7, u_kt: 49}}
    conv2d_29.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer1.0.conv2.weight_fork_clone1325],
         t: 1, mblock: [14, 2], ublock: [7, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 1}}
    _fused_op_1: {type: fused_op, grid_loc: [1, 7], grid_size: [1, 1], inputs: [conv2d_29.dc.conv2d.1.dc.matmul.11, conv2d_29.dc.conv2d.3.dc.matmul.11, conv2d_29.dc.conv2d.5.dc.matmul.11, input_1_add_30, input_1_add_30_fork_clone1095],
         t: 1, mblock: [49, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 98}], input_3_tms: [broadcast: {r: 98}],
         attributes: {fused_op_id: 1}}
    conv2d_43.dc.matmul.8: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [_fused_op_1, layer1.0.conv3.weight, input_1_add_44_fork_clone1005],
         t: 2, mblock: [49, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 98}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 1, min_buffer_input: 0, u_kt: 2}}
    conv2d_56.dc.matmul.8: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [insert_queue0, layer1.0.downsample.0.weight, input_1_add_57_fork_clone1010],
         t: 2, mblock: [49, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 98}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vstack: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 1, min_buffer_input: 0, u_kt: 2}}
    add_69: {type: add, grid_loc: [2, 1], grid_size: [1, 1], inputs: [conv2d_43.dc.matmul.8, conv2d_56.dc.matmul.8],
         t: 2, mblock: [49, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_71.dc.matmul.8: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [buf_add_69_0, layer1.1.conv1.weight, input_1_add_72_fork_clone1130],
         t: 2, mblock: [49, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 98}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, m_k: 2, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 4}}
    conv2d_85.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 0], grid_size: [1, 2], inputs: [lc.input_tensor.conv2d_85.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_71.dc.matmul.8, lc.input_tensor.conv2d_85.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [42, 1], ublock: [7, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 2], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 7, u_kt: 49}}
    conv2d_85.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [conv2d_85.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer1.1.conv2.weight_fork_clone1374],
         t: 1, mblock: [14, 2], ublock: [7, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 1}}
    conv2d_85.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 3], grid_size: [1, 2], inputs: [lc.input_tensor.conv2d_85.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_71.dc.matmul.8, lc.input_tensor.conv2d_85.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [42, 1], ublock: [7, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 2], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 7, u_kt: 49}}
    conv2d_85.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [conv2d_85.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer1.1.conv2.weight],
         t: 1, mblock: [14, 2], ublock: [7, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 1}}
    conv2d_85.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_85.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_71.dc.matmul.8, lc.input_tensor.conv2d_85.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [6, 2], ublock: [7, 1], buf_size_mb: 2, input_buf_min_size_tiles: [0, 686, 0], ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 2],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 12, sparse_tile_ptr_bits: 5, u_kt: 49}}
    conv2d_85.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [conv2d_85.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer1.1.conv2.weight_fork_clone1372],
         t: 7, mblock: [2, 2], ublock: [7, 1], buf_size_mb: 2, input_buf_min_size_tiles: [840, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 1}}
    _fused_op_2: {type: fused_op, grid_loc: [3, 3], grid_size: [1, 1], inputs: [conv2d_85.dc.conv2d.1.dc.matmul.11, conv2d_85.dc.conv2d.3.dc.matmul.11, conv2d_85.dc.conv2d.5.dc.matmul.11, input_1_add_86, input_1_add_86_fork_clone1047],
         t: 7, mblock: [7, 1], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [0, 756, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 98}, vslice: 7], input_3_tms: [broadcast: {r: 98}, vslice: 7], input_2_tms: [vslice: 7], input_0_tms: [vslice: 7],
         attributes: {fused_op_id: 2}}
    conv2d_99.dc.matmul.8: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [_fused_op_2, layer1.1.conv3.weight, input_1_add_100_fork_clone952],
         t: 7, mblock: [7, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 98}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    add_112: {type: add, grid_loc: [3, 5], grid_size: [1, 1], inputs: [conv2d_99.dc.matmul.8, insert_queue1],
         t: 7, mblock: [7, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 2, vslice: 7],
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_114.dc.matmul.8: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [add_112, layer1.2.conv1.weight, input_1_add_115_fork_clone1083],
         t: 7, mblock: [7, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 98}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 2, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 4}}
    conv2d_128.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 0], grid_size: [1, 2], inputs: [lc.input_tensor.conv2d_128.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_114.dc.matmul.8, lc.input_tensor.conv2d_128.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [6, 1], ublock: [7, 1], buf_size_mb: 2, input_buf_min_size_tiles: [0, 819, 0], ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 7], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 5, u_kt: 7}}
    conv2d_128.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [conv2d_128.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer1.2.conv2.weight_fork_clone1408],
         t: 7, mblock: [2, 2], ublock: [7, 1], buf_size_mb: 2, input_buf_min_size_tiles: [840, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 1}}
    conv2d_128.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 3], grid_size: [1, 2], inputs: [lc.input_tensor.conv2d_128.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_114.dc.matmul.8, lc.input_tensor.conv2d_128.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 7, mblock: [6, 1], ublock: [7, 1], buf_size_mb: 2, input_buf_min_size_tiles: [0, 819, 0], ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 7], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 15, sparse_tile_ptr_bits: 5, u_kt: 7}}
    conv2d_128.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [conv2d_128.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer1.2.conv2.weight],
         t: 7, mblock: [2, 2], ublock: [7, 1], buf_size_mb: 2, input_buf_min_size_tiles: [840, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 1}}
    conv2d_128.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 6], grid_size: [1, 2], inputs: [lc.input_tensor.conv2d_128.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_114.dc.matmul.8, lc.input_tensor.conv2d_128.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [42, 1], ublock: [7, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 7], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 12, sparse_tile_ptr_bits: 7, u_kt: 7}}
    conv2d_128.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [conv2d_128.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer1.2.conv2.weight_fork_clone1406],
         t: 1, mblock: [14, 2], ublock: [7, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 6, min_buffer_input: 0, u_kt: 1}}
    _fused_op_3: {type: fused_op, grid_loc: [5, 3], grid_size: [1, 1], inputs: [conv2d_128.dc.conv2d.1.dc.matmul.11, conv2d_128.dc.conv2d.3.dc.matmul.11, conv2d_128.dc.conv2d.5.dc.matmul.11, input_1_add_129, input_1_add_129_fork_clone993],
         t: 7, mblock: [7, 1], ublock: [2, 2], buf_size_mb: 2, input_buf_min_size_tiles: [112, 0, 112, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [broadcast: {r: 98}, vslice: 7], input_3_tms: [broadcast: {r: 98}, vslice: 7], input_1_tms: [vslice: 7],
         attributes: {fused_op_id: 2}}
    conv2d_142.dc.matmul.8: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [_fused_op_3, layer1.2.conv3.weight, input_1_add_143_fork_clone899],
         t: 7, mblock: [7, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 98}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    buffer_0_add_112_buffer_0_add_112_buffer_0_add_112_buffer_0_add_112_add_155: {type: nop, grid_loc: [3, 7], grid_size: [1, 1], inputs: [add_112],
         t: 7, mblock: [7, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [688], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_112_buffer_0_add_112_buffer_0_add_112_add_155: {type: nop, grid_loc: [5, 1], grid_size: [1, 1], inputs: [buffer_0_add_112_buffer_0_add_112_buffer_0_add_112_buffer_0_add_112_add_155],
         t: 7, mblock: [7, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [688], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_112_buffer_0_add_112_add_155: {type: nop, grid_loc: [5, 2], grid_size: [1, 1], inputs: [buffer_0_add_112_buffer_0_add_112_buffer_0_add_112_add_155],
         t: 7, mblock: [7, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [688], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_112_add_155: {type: nop, grid_loc: [5, 4], grid_size: [1, 1], inputs: [buffer_0_add_112_buffer_0_add_112_add_155],
         t: 7, mblock: [7, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [688], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_155: {type: add, grid_loc: [5, 6], grid_size: [1, 1], inputs: [conv2d_142.dc.matmul.8, buffer_0_add_112_add_155],
         t: 7, mblock: [7, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 112], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_157.dc.matmul.8: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [add_155, layer2.0.conv1.weight, input_1_add_158_fork_clone965],
         t: 7, mblock: [7, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [728, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 98}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 2, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 4}}
    conv2d_171.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_171.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_157.dc.matmul.8, lc.input_tensor.conv2d_171.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 25, mblock: [1, 2], ublock: [3, 2], buf_size_mb: 2, input_buf_min_size_tiles: [0, 840, 0], ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 70, sparse_tile_ptr_bits: 8, u_kt: 7}}
    conv2d_171.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [conv2d_171.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer2.0.conv2.weight_fork_clone1442],
         t: 25, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [828, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 25}, hslice: 25], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_171.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_171.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_157.dc.matmul.8, lc.input_tensor.conv2d_171.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 25, mblock: [1, 2], ublock: [3, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 67, sparse_tile_ptr_bits: 8, u_kt: 7}}
    conv2d_171.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [conv2d_171.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer2.0.conv2.weight],
         t: 25, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 25}, hslice: 25], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_171.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_171.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_157.dc.matmul.8, lc.input_tensor.conv2d_171.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 25, mblock: [1, 2], ublock: [3, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 14, num_index_tiles: 1, num_sparse_tiles: 68, sparse_tile_ptr_bits: 8, u_kt: 7}}
    conv2d_171.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [conv2d_171.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer2.0.conv2.weight_fork_clone1440],
         t: 25, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 25}, hslice: 25], input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    _fused_op_4: {type: fused_op, grid_loc: [7, 1], grid_size: [1, 1], inputs: [conv2d_171.dc.conv2d.1.dc.matmul.11, conv2d_171.dc.conv2d.3.dc.matmul.11, conv2d_171.dc.conv2d.5.dc.matmul.11, input_1_add_172, input_1_add_172_fork_clone871],
         t: 25, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 32, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_4_tms: [vslice: 25], input_3_tms: [vslice: 25],
         attributes: {fused_op_id: 4}}
    conv2d_185.dc.matmul.8: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [_fused_op_4, layer2.0.conv3.weight, input_1_add_186_fork_clone769],
         t: 25, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 25}, vslice: 25], input_1_tms: [broadcast: {c: 25}, hslice: 25],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 2, min_buffer_input: 0, u_kt: 2}}
    conv2d_198.dc.sparse_matmul.9.lc2: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_198.dc.sparse_matmul.9.0, add_155, lc.input_tensor.conv2d_198.dc.sparse_matmul.9.1],
         t: 1, mblock: [25, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 23, sparse_tile_ptr_bits: 6, u_kt: 14}}
    conv2d_198.dc.matmul.10: {type: matmul, grid_loc: [6, 1], grid_size: [1, 2], inputs: [conv2d_198.dc.sparse_matmul.9.lc2, layer2.0.downsample.0.weight, input_1_add_199_fork_clone774],
         t: 1, mblock: [25, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 25}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 2, min_buffer_input: 0, u_kt: 4}}
    add_211: {type: add, grid_loc: [7, 3], grid_size: [1, 1], inputs: [conv2d_185.dc.matmul.8, conv2d_198.dc.matmul.10],
         t: 25, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [12, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [vslice: 25],
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_213.dc.matmul.8: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [add_211, layer2.1.conv1.weight, input_1_add_214_fork_clone913],
         t: 25, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 25}, vslice: 25], input_1_tms: [broadcast: {c: 25}, hslice: 25],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 4, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 4}}
    conv2d_227.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 6], grid_size: [1, 2], inputs: [lc.input_tensor.conv2d_227.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_213.dc.matmul.8, lc.input_tensor.conv2d_227.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [25, 1], ublock: [3, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 25], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 25, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_227.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [conv2d_227.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer2.1.conv2.weight_fork_clone1491],
         t: 1, mblock: [5, 4], ublock: [5, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_227.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [8, 1], grid_size: [1, 2], inputs: [lc.input_tensor.conv2d_227.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_213.dc.matmul.8, lc.input_tensor.conv2d_227.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [25, 1], ublock: [3, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 25], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 25, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_227.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [conv2d_227.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer2.1.conv2.weight],
         t: 1, mblock: [5, 4], ublock: [5, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    conv2d_227.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [8, 4], grid_size: [1, 2], inputs: [lc.input_tensor.conv2d_227.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_213.dc.matmul.8, lc.input_tensor.conv2d_227.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [25, 1], ublock: [3, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 25], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 25, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_227.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [conv2d_227.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer2.1.conv2.weight_fork_clone1489],
         t: 1, mblock: [5, 4], ublock: [5, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    _fused_op_5: {type: fused_op, grid_loc: [9, 1], grid_size: [1, 1], inputs: [conv2d_227.dc.conv2d.1.dc.matmul.11, conv2d_227.dc.conv2d.3.dc.matmul.11, conv2d_227.dc.conv2d.5.dc.matmul.11, input_1_add_228, input_1_add_228_fork_clone815],
         t: 1, mblock: [25, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {fused_op_id: 5}}
    conv2d_241.dc.matmul.8: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [_fused_op_5, layer2.1.conv3.weight, input_1_add_242_fork_clone714],
         t: 25, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 25}, vslice: 25], input_1_tms: [broadcast: {c: 25}, hslice: 25], input_0_tms: [vslice: 25],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 2, min_buffer_input: 0, u_kt: 2}}
    buffer_0_add_211_buffer_0_add_211_buffer_0_add_211_buffer_1_add_211_buffer_2_add_211_add_254: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [add_211],
         t: 25, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_211_buffer_0_add_211_buffer_1_add_211_buffer_2_add_211_add_254: {type: nop, grid_loc: [8, 7], grid_size: [1, 1], inputs: [buffer_0_add_211_buffer_0_add_211_buffer_0_add_211_buffer_1_add_211_buffer_2_add_211_add_254],
         t: 25, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_211_buffer_1_add_211_buffer_2_add_211_add_254: {type: nop, grid_loc: [9, 0], grid_size: [1, 1], inputs: [buffer_0_add_211_buffer_0_add_211_buffer_1_add_211_buffer_2_add_211_add_254],
         t: 25, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_211_buffer_2_add_211_add_254: {type: nop, grid_loc: [9, 2], grid_size: [1, 1], inputs: [buffer_0_add_211_buffer_1_add_211_buffer_2_add_211_add_254],
         t: 25, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_211_buffer_2_add_211_add_254: {type: nop, grid_loc: [9, 4], grid_size: [1, 1], inputs: [buffer_1_add_211_buffer_2_add_211_add_254],
         t: 25, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_211_add_254: {type: nop, grid_loc: [9, 5], grid_size: [1, 1], inputs: [buffer_0_add_211_buffer_2_add_211_add_254],
         t: 25, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_211_add_254: {type: nop, grid_loc: [9, 6], grid_size: [1, 1], inputs: [buffer_2_add_211_add_254],
         t: 25, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_211_add_254: {type: nop, grid_loc: [9, 7], grid_size: [1, 1], inputs: [buffer_1_add_211_add_254],
         t: 25, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_0_2_temporal_epoch_2:
    target_device: 0
    input_count: 64
    add_254: {type: add, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_conv2d_241.dc.matmul.8_0, e2e_buffer_0_add_211_add_254_0],
         t: 1, mblock: [25, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 25], input_0_tms: [vstack: 25],
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_256.dc.matmul.8: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [add_254, layer2.2.conv1.weight, input_1_add_257_fork_clone859],
         t: 1, mblock: [25, 1], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [512, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 25}],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 16}}
    conv2d_270.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_270.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_256.dc.matmul.8, lc.input_tensor.conv2d_270.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 4], ublock: [5, 1], buf_size_mb: 2, input_buf_min_size_tiles: [0, 300, 0], ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 6, u_kt: 25}}
    conv2d_270.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [conv2d_270.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer2.2.conv2.weight_fork_clone1525],
         t: 1, mblock: [5, 4], ublock: [5, 1], buf_size_mb: 2, input_buf_min_size_tiles: [540, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 12}}
    conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_256.dc.matmul.8, lc.input_tensor.conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 4], ublock: [5, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 24, sparse_tile_ptr_bits: 6, u_kt: 25}}
    conv2d_270.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer2.2.conv2.weight],
         t: 1, mblock: [5, 4], ublock: [5, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 12}}
    conv2d_270.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_270.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_256.dc.matmul.8, lc.input_tensor.conv2d_270.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 4], ublock: [5, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 6, u_kt: 25}}
    conv2d_270.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [conv2d_270.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer2.2.conv2.weight_fork_clone1523],
         t: 1, mblock: [5, 4], ublock: [5, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 12}}
    _fused_op_6: {type: fused_op, grid_loc: [1, 3], grid_size: [1, 1], inputs: [conv2d_270.dc.conv2d.1.dc.matmul.11, conv2d_270.dc.conv2d.3.dc.matmul.11, conv2d_270.dc.conv2d.5.dc.matmul.11, input_1_add_271, input_1_add_271_fork_clone757],
         t: 1, mblock: [25, 1], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 440, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {fused_op_id: 5}}
    conv2d_284.dc.matmul.8: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [_fused_op_6, layer2.2.conv3.weight, input_1_add_285_fork_clone663],
         t: 5, mblock: [5, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 25}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    buffer_7_add_254_add_297: {type: nop, grid_loc: [0, 2], grid_size: [1, 1], inputs: [add_254],
         t: 1, mblock: [25, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_6_add_254_add_297: {type: nop, grid_loc: [1, 1], grid_size: [1, 1], inputs: [buffer_7_add_254_add_297],
         t: 1, mblock: [25, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_5_add_254_add_297: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [buffer_6_add_254_add_297],
         t: 1, mblock: [25, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_4_add_254_add_297: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [buffer_5_add_254_add_297],
         t: 1, mblock: [25, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3_add_254_add_297: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [buffer_4_add_254_add_297],
         t: 1, mblock: [25, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_254_add_297: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [buffer_3_add_254_add_297],
         t: 1, mblock: [25, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_254_add_297: {type: nop, grid_loc: [2, 0], grid_size: [1, 1], inputs: [buffer_2_add_254_add_297],
         t: 1, mblock: [25, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_254_add_297: {type: nop, grid_loc: [2, 1], grid_size: [1, 1], inputs: [buffer_1_add_254_add_297],
         t: 1, mblock: [25, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_297: {type: add, grid_loc: [2, 2], grid_size: [1, 1], inputs: [conv2d_284.dc.matmul.8, buffer_0_add_254_add_297],
         t: 5, mblock: [5, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [vslice: 5],
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_299.dc.matmul.8: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [add_297, layer2.3.conv1.weight, input_1_add_300_fork_clone803],
         t: 5, mblock: [5, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 25}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 16}}
    conv2d_313.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 5], grid_size: [3, 1], inputs: [lc.input_tensor.conv2d_313.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_299.dc.matmul.8, lc.input_tensor.conv2d_313.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1], grid_transpose: true,
         t: 1, mblock: [5, 4], ublock: [5, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 25, num_index_tiles: 1, num_sparse_tiles: 12, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_313.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [conv2d_313.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer2.3.conv2.weight_fork_clone1559],
         t: 1, mblock: [5, 4], ublock: [5, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 12}}
    conv2d_313.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 1], grid_size: [3, 1], inputs: [lc.input_tensor.conv2d_313.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_299.dc.matmul.8, lc.input_tensor.conv2d_313.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1], grid_transpose: true,
         t: 1, mblock: [5, 4], ublock: [5, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 25, num_index_tiles: 1, num_sparse_tiles: 12, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_313.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [conv2d_313.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer2.3.conv2.weight],
         t: 1, mblock: [5, 4], ublock: [5, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 12}}
    conv2d_313.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_313.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_299.dc.matmul.8, lc.input_tensor.conv2d_313.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [15, 4], ublock: [5, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 21, sparse_tile_ptr_bits: 6, u_kt: 5}}
    conv2d_313.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [conv2d_313.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer2.3.conv2.weight_fork_clone1557],
         t: 1, mblock: [5, 4], ublock: [5, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 12}}
    _fused_op_7: {type: fused_op, grid_loc: [4, 1], grid_size: [1, 1], inputs: [conv2d_313.dc.conv2d.1.dc.matmul.11, conv2d_313.dc.conv2d.3.dc.matmul.11, conv2d_313.dc.conv2d.5.dc.matmul.11, input_1_add_314, input_1_add_314_fork_clone702],
         t: 1, mblock: [25, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {fused_op_id: 5}}
    conv2d_327.dc.matmul.8: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [_fused_op_7, layer2.3.conv3.weight, input_1_add_328_fork_clone610],
         t: 5, mblock: [5, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 25}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    buffer_7_add_297_add_340: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [add_297],
         t: 5, mblock: [5, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [756], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_6_add_297_add_340: {type: nop, grid_loc: [3, 7], grid_size: [1, 1], inputs: [buffer_7_add_297_add_340],
         t: 5, mblock: [5, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [756], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_5_add_297_add_340: {type: nop, grid_loc: [4, 0], grid_size: [1, 1], inputs: [buffer_6_add_297_add_340],
         t: 5, mblock: [5, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [756], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_4_add_297_add_340: {type: nop, grid_loc: [4, 2], grid_size: [1, 1], inputs: [buffer_5_add_297_add_340],
         t: 5, mblock: [5, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [756], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3_add_297_add_340: {type: nop, grid_loc: [4, 4], grid_size: [1, 1], inputs: [buffer_4_add_297_add_340],
         t: 5, mblock: [5, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [756], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_297_add_340: {type: nop, grid_loc: [4, 5], grid_size: [1, 1], inputs: [buffer_3_add_297_add_340],
         t: 5, mblock: [5, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [756], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_297_add_340: {type: nop, grid_loc: [4, 6], grid_size: [1, 1], inputs: [buffer_2_add_297_add_340],
         t: 5, mblock: [5, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [672], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_297_add_340: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [buffer_1_add_297_add_340],
         t: 5, mblock: [5, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_340: {type: add, grid_loc: [5, 0], grid_size: [1, 1], inputs: [conv2d_327.dc.matmul.8, buffer_0_add_297_add_340],
         t: 5, mblock: [5, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_342.dc.matmul.8: {type: matmul, grid_loc: [5, 1], grid_size: [5, 1], inputs: [add_340, layer3.0.conv1.weight, input_1_add_343_fork_clone675], grid_transpose: true,
         t: 5, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 25}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 16, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 1}}
    conv2d_356.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_356.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_342.dc.matmul.8, lc.input_tensor.conv2d_356.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 25, num_index_tiles: 1, num_sparse_tiles: 94, sparse_tile_ptr_bits: 8, u_kt: 1}}
    conv2d_356.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [conv2d_356.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer3.0.conv2.weight_fork_clone1593],
         t: 1, mblock: [7, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    conv2d_356.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_356.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_342.dc.matmul.8, lc.input_tensor.conv2d_356.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 25, num_index_tiles: 1, num_sparse_tiles: 86, sparse_tile_ptr_bits: 8, u_kt: 1}}
    conv2d_356.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [conv2d_356.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer3.0.conv2.weight],
         t: 1, mblock: [7, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    conv2d_356.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_356.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_342.dc.matmul.8, lc.input_tensor.conv2d_356.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 25, num_index_tiles: 1, num_sparse_tiles: 91, sparse_tile_ptr_bits: 8, u_kt: 1}}
    conv2d_356.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [conv2d_356.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer3.0.conv2.weight_fork_clone1591],
         t: 1, mblock: [7, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    _fused_op_8: {type: fused_op, grid_loc: [7, 0], grid_size: [1, 1], inputs: [conv2d_356.dc.conv2d.1.dc.matmul.11, conv2d_356.dc.conv2d.3.dc.matmul.11, conv2d_356.dc.conv2d.5.dc.matmul.11, input_1_add_357, input_1_add_357_fork_clone580],
         t: 1, mblock: [7, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {fused_op_id: 8}}
    conv2d_370.dc.matmul.8: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [_fused_op_8, layer3.0.conv3.weight, input_1_add_371_fork_clone466],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 1, min_buffer_input: 0, u_kt: 8}}
    buffer_0_add_340_conv2d_383.dc.sparse_matmul.9.lc2: {type: nop, grid_loc: [5, 6], grid_size: [1, 1], inputs: [add_340],
         t: 1, mblock: [25, 4], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [116], ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vstack: 5]}
    conv2d_383.dc.sparse_matmul.9.lc2: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_383.dc.sparse_matmul.9.0, buffer_0_add_340_conv2d_383.dc.sparse_matmul.9.lc2, lc.input_tensor.conv2d_383.dc.sparse_matmul.9.1],
         t: 1, mblock: [7, 2], ublock: [1, 8], buf_size_mb: 2, input_buf_min_size_tiles: [0, 640, 0], ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 5, num_index_tiles: 1, num_sparse_tiles: 31, sparse_tile_ptr_bits: 6, u_kt: 5}}
    conv2d_383.dc.matmul.10: {type: matmul, grid_loc: [6, 6], grid_size: [1, 2], inputs: [conv2d_383.dc.sparse_matmul.9.lc2, layer3.0.downsample.0.weight, input_1_add_384_fork_clone471],
         t: 1, mblock: [7, 2], ublock: [1, 8], buf_size_mb: 2, input_buf_min_size_tiles: [160, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 7}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 2, min_buffer_input: 0, u_kt: 8}}
    add_396: {type: add, grid_loc: [7, 2], grid_size: [1, 1], inputs: [conv2d_370.dc.matmul.8, conv2d_383.dc.matmul.10],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 488], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [vslice: 7],
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_398.dc.matmul.8: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [add_396, layer3.1.conv1.weight, input_1_add_399_fork_clone624],
         t: 7, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 32}}
    conv2d_412.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_412.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_398.dc.matmul.8, lc.input_tensor.conv2d_412.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_412.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [conv2d_412.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer3.1.conv2.weight_fork_clone1642],
         t: 1, mblock: [7, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    conv2d_412.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_412.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_398.dc.matmul.8, lc.input_tensor.conv2d_412.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_412.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [conv2d_412.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer3.1.conv2.weight],
         t: 1, mblock: [7, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    conv2d_412.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_412.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_398.dc.matmul.8, lc.input_tensor.conv2d_412.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_412.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [conv2d_412.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer3.1.conv2.weight_fork_clone1640],
         t: 1, mblock: [7, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    _fused_op_9: {type: fused_op, grid_loc: [8, 5], grid_size: [1, 1], inputs: [conv2d_412.dc.conv2d.1.dc.matmul.11, conv2d_412.dc.conv2d.3.dc.matmul.11, conv2d_412.dc.conv2d.5.dc.matmul.11, input_1_add_413, input_1_add_413_fork_clone520],
         t: 1, mblock: [7, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {fused_op_id: 8}}
    conv2d_426.dc.matmul.8: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [_fused_op_9, layer3.1.conv3.weight, input_1_add_427_fork_clone401],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 1, min_buffer_input: 0, u_kt: 8}}
    buffer_0_add_396_buffer_0_add_396_buffer_0_add_396_buffer_0_add_396_add_439: {type: nop, grid_loc: [7, 4], grid_size: [1, 1], inputs: [add_396],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [852], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_396_buffer_0_add_396_buffer_0_add_396_add_439: {type: nop, grid_loc: [8, 3], grid_size: [1, 1], inputs: [buffer_0_add_396_buffer_0_add_396_buffer_0_add_396_buffer_0_add_396_add_439],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [852], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_396_buffer_0_add_396_add_439: {type: nop, grid_loc: [8, 4], grid_size: [1, 1], inputs: [buffer_0_add_396_buffer_0_add_396_buffer_0_add_396_add_439],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [852], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_396_add_439: {type: nop, grid_loc: [8, 6], grid_size: [1, 1], inputs: [buffer_0_add_396_buffer_0_add_396_add_439],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [852], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_439: {type: add, grid_loc: [9, 0], grid_size: [1, 1], inputs: [conv2d_426.dc.matmul.8, buffer_0_add_396_add_439],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 472], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_441.dc.matmul.8: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [add_439, layer3.2.conv1.weight, input_1_add_442_fork_clone568],
         t: 7, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 32}}
    conv2d_455.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_455.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_441.dc.matmul.8, lc.input_tensor.conv2d_455.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_455.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [9, 4], grid_size: [1, 1], inputs: [conv2d_455.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer3.2.conv2.weight_fork_clone1676],
         t: 1, mblock: [7, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    conv2d_455.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_455.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_441.dc.matmul.8, lc.input_tensor.conv2d_455.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_455.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [conv2d_455.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer3.2.conv2.weight],
         t: 1, mblock: [7, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [9, 7], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_441.dc.matmul.8, lc.input_tensor.conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 6, u_kt: 1}}
    buffer_0_add_439_buffer_0_add_439_buffer_0_add_439_add_482: {type: nop, grid_loc: [9, 2], grid_size: [1, 1], inputs: [add_439],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_0_3_temporal_epoch_3:
    target_device: 0
    input_count: 64
    conv2d_455.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [7, 1], inputs: [e2e_conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2_0, layer3.2.conv2.weight_fork_clone1674],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    _fused_op_10: {type: fused_op, grid_loc: [0, 3], grid_size: [7, 1], inputs: [e2e_conv2d_455.dc.conv2d.1.dc.matmul.11_0, conv2d_455.dc.conv2d.3.dc.matmul.11, e2e_conv2d_455.dc.conv2d.5.dc.matmul.11_0, input_1_add_456, input_1_add_456_fork_clone454],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {fused_op_id: 10}}
    conv2d_469.dc.matmul.8: {type: matmul, grid_loc: [0, 4], grid_size: [7, 1], inputs: [_fused_op_10, layer3.2.conv3.weight, input_1_add_470_fork_clone344],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 7}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 1, min_buffer_input: 0, u_kt: 8}}
    buffer_0_add_439_buffer_0_add_439_add_482: {type: nop, grid_loc: [0, 0], grid_size: [7, 1], inputs: [e2e_buffer_0_add_439_buffer_0_add_439_buffer_0_add_439_add_482_0],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [vstack: 7]}
    buffer_0_add_439_add_482: {type: nop, grid_loc: [0, 2], grid_size: [7, 1], inputs: [buffer_0_add_439_buffer_0_add_439_add_482],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_482: {type: add, grid_loc: [0, 5], grid_size: [7, 1], inputs: [conv2d_469.dc.matmul.8, buffer_0_add_439_add_482],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_484.dc.matmul.8: {type: matmul, grid_loc: [0, 6], grid_size: [7, 1], inputs: [add_482, layer3.3.conv1.weight, input_1_add_485_fork_clone508],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 7}],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 32}}
    conv2d_498.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 0], grid_size: [7, 2], inputs: [lc.input_tensor.conv2d_498.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_484.dc.matmul.8, lc.input_tensor.conv2d_498.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1], grid_transpose: true,
         t: 1, mblock: [1, 2], ublock: [3, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {c: 2}], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_498.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [9, 0], grid_size: [7, 1], inputs: [conv2d_498.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer3.3.conv2.weight_fork_clone1710], grid_transpose: true,
         t: 1, mblock: [1, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    buffer_0_add_482_buffer_0_add_482_buffer_0_add_482_buffer_0_add_482_add_525: {type: nop, grid_loc: [0, 7], grid_size: [7, 1], inputs: [add_482],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_0_4_temporal_epoch_4:
    target_device: 0
    input_count: 64
    conv2d_498.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_498.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_conv2d_484.dc.matmul.8_0, lc.input_tensor.conv2d_498.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, u_kt: 7}}
    conv2d_498.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [conv2d_498.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer3.3.conv2.weight],
         t: 1, mblock: [7, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    conv2d_498.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_498.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_conv2d_484.dc.matmul.8_0, lc.input_tensor.conv2d_498.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 6, u_kt: 7}}
    conv2d_498.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [conv2d_498.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer3.3.conv2.weight_fork_clone1708],
         t: 1, mblock: [7, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    _fused_op_11: {type: fused_op, grid_loc: [0, 6], grid_size: [1, 1], inputs: [conv2d_498.dc.conv2d.1.dc.matmul.11, conv2d_498.dc.conv2d.3.dc.matmul.11, e2e_conv2d_498.dc.conv2d.5.dc.matmul.11_0, input_1_add_499, input_1_add_499_fork_clone389],
         t: 1, mblock: [7, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {fused_op_id: 8}}
    conv2d_512.dc.matmul.8: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [_fused_op_11, layer3.3.conv3.weight, input_1_add_513_fork_clone291],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 1, min_buffer_input: 0, u_kt: 8}}
    buffer_0_add_482_buffer_0_add_482_buffer_0_add_482_add_525: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e_buffer_0_add_482_buffer_0_add_482_buffer_0_add_482_buffer_0_add_482_add_525_0],
         t: 1, mblock: [7, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_482_buffer_0_add_482_add_525: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [buffer_0_add_482_buffer_0_add_482_buffer_0_add_482_add_525],
         t: 1, mblock: [7, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_482_add_525: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [buffer_0_add_482_buffer_0_add_482_add_525],
         t: 1, mblock: [7, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_525: {type: add, grid_loc: [1, 1], grid_size: [1, 1], inputs: [conv2d_512.dc.matmul.8, buffer_0_add_482_add_525],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [vslice: 7],
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_527.dc.matmul.8: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [add_525, layer3.4.conv1.weight, input_1_add_528_fork_clone442],
         t: 7, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 32}}
    conv2d_541.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_541.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_527.dc.matmul.8, lc.input_tensor.conv2d_541.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_541.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [conv2d_541.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer3.4.conv2.weight_fork_clone1744],
         t: 1, mblock: [7, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    conv2d_541.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_541.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_527.dc.matmul.8, lc.input_tensor.conv2d_541.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_541.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [conv2d_541.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer3.4.conv2.weight],
         t: 1, mblock: [7, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    conv2d_541.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_541.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_527.dc.matmul.8, lc.input_tensor.conv2d_541.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_541.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [conv2d_541.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer3.4.conv2.weight_fork_clone1742],
         t: 1, mblock: [7, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    _fused_op_12: {type: fused_op, grid_loc: [2, 4], grid_size: [1, 1], inputs: [conv2d_541.dc.conv2d.1.dc.matmul.11, conv2d_541.dc.conv2d.3.dc.matmul.11, conv2d_541.dc.conv2d.5.dc.matmul.11, input_1_add_542, input_1_add_542_fork_clone332],
         t: 1, mblock: [7, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {fused_op_id: 8}}
    conv2d_555.dc.matmul.8: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [_fused_op_12, layer3.4.conv3.weight, input_1_add_556_fork_clone240],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 1, min_buffer_input: 0, u_kt: 8}}
    buffer_0_add_525_buffer_0_add_525_buffer_0_add_525_buffer_0_add_525_add_568: {type: nop, grid_loc: [1, 3], grid_size: [1, 1], inputs: [add_525],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [852], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_525_buffer_0_add_525_buffer_0_add_525_add_568: {type: nop, grid_loc: [2, 2], grid_size: [1, 1], inputs: [buffer_0_add_525_buffer_0_add_525_buffer_0_add_525_buffer_0_add_525_add_568],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [852], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_525_buffer_0_add_525_add_568: {type: nop, grid_loc: [2, 3], grid_size: [1, 1], inputs: [buffer_0_add_525_buffer_0_add_525_buffer_0_add_525_add_568],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [852], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_525_add_568: {type: nop, grid_loc: [2, 5], grid_size: [1, 1], inputs: [buffer_0_add_525_buffer_0_add_525_add_568],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [852], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_568: {type: add, grid_loc: [2, 7], grid_size: [1, 1], inputs: [conv2d_555.dc.matmul.8, buffer_0_add_525_add_568],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 472], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_570.dc.matmul.8: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [add_568, layer3.5.conv1.weight, input_1_add_571_fork_clone377],
         t: 7, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 32}}
    conv2d_584.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_584.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_570.dc.matmul.8, lc.input_tensor.conv2d_584.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_584.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [conv2d_584.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer3.5.conv2.weight_fork_clone1778],
         t: 1, mblock: [7, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    conv2d_584.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_584.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_570.dc.matmul.8, lc.input_tensor.conv2d_584.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 28, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_584.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [conv2d_584.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer3.5.conv2.weight],
         t: 1, mblock: [7, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    conv2d_584.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_584.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_570.dc.matmul.8, lc.input_tensor.conv2d_584.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [21, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 19, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_584.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [conv2d_584.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer3.5.conv2.weight_fork_clone1776],
         t: 1, mblock: [7, 1], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 24}}
    _fused_op_13: {type: fused_op, grid_loc: [4, 2], grid_size: [1, 1], inputs: [conv2d_584.dc.conv2d.1.dc.matmul.11, conv2d_584.dc.conv2d.3.dc.matmul.11, conv2d_584.dc.conv2d.5.dc.matmul.11, input_1_add_585, input_1_add_585_fork_clone279],
         t: 1, mblock: [7, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {fused_op_id: 8}}
    conv2d_598.dc.matmul.8: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [_fused_op_13, layer3.5.conv3.weight, input_1_add_599_fork_clone190],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7], input_0_tms: [vslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 1, min_buffer_input: 0, u_kt: 8}}
    buffer_0_add_568_buffer_0_add_568_buffer_0_add_568_buffer_0_add_568_add_611: {type: nop, grid_loc: [3, 1], grid_size: [1, 1], inputs: [add_568],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [852], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_568_buffer_0_add_568_buffer_0_add_568_add_611: {type: nop, grid_loc: [4, 0], grid_size: [1, 1], inputs: [buffer_0_add_568_buffer_0_add_568_buffer_0_add_568_buffer_0_add_568_add_611],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [852], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_568_buffer_0_add_568_add_611: {type: nop, grid_loc: [4, 1], grid_size: [1, 1], inputs: [buffer_0_add_568_buffer_0_add_568_buffer_0_add_568_add_611],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [852], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_568_add_611: {type: nop, grid_loc: [4, 3], grid_size: [1, 1], inputs: [buffer_0_add_568_buffer_0_add_568_add_611],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [852], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_611: {type: add, grid_loc: [4, 5], grid_size: [1, 1], inputs: [conv2d_598.dc.matmul.8, buffer_0_add_568_add_611],
         t: 7, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 472], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_613.dc.matmul.8: {type: matmul, grid_loc: [4, 6], grid_size: [1, 2], inputs: [add_611, layer4.0.conv1.weight, input_1_add_614_fork_clone252],
         t: 7, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 7}, vslice: 7], input_1_tms: [broadcast: {c: 7}, hslice: 7],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 32}}
    conv2d_627.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 3], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_627.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_613.dc.matmul.8, lc.input_tensor.conv2d_627.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 14, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_627.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [5, 4], grid_size: [2, 1], inputs: [conv2d_627.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer4.0.conv2.weight_fork_clone1812],
         t: 1, mblock: [1, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 12, min_buffer_input: 0, u_kt: 4}}
    conv2d_627.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 5], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_627.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_613.dc.matmul.8, lc.input_tensor.conv2d_627.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 12, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_627.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [5, 6], grid_size: [2, 1], inputs: [conv2d_627.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer4.0.conv2.weight],
         t: 1, mblock: [1, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 12, min_buffer_input: 0, u_kt: 4}}
    conv2d_627.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [5, 7], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_627.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_613.dc.matmul.8, lc.input_tensor.conv2d_627.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 12, sparse_tile_ptr_bits: 6, u_kt: 1}}
    conv2d_627.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [7, 0], grid_size: [2, 1], inputs: [conv2d_627.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer4.0.conv2.weight_fork_clone1810],
         t: 1, mblock: [1, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 12, min_buffer_input: 0, u_kt: 4}}
    _fused_op_14: {type: fused_op, grid_loc: [7, 1], grid_size: [2, 1], inputs: [conv2d_627.dc.conv2d.1.dc.matmul.11, conv2d_627.dc.conv2d.3.dc.matmul.11, conv2d_627.dc.conv2d.5.dc.matmul.11, input_1_add_628, input_1_add_628_fork_clone163],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {fused_op_id: 14}}
    conv2d_641.dc.matmul.8: {type: matmul, grid_loc: [7, 2], grid_size: [2, 2], inputs: [_fused_op_14, layer4.0.conv3.weight, input_1_add_642_fork_clone82],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 2}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 0, u_kt: 1}}
    conv2d_654.dc.sparse_matmul.9.lc2: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_654.dc.sparse_matmul.9.0, add_611, lc.input_tensor.conv2d_654.dc.sparse_matmul.9.1],
         t: 1, mblock: [1, 4], ublock: [1, 8], buf_size_mb: 2, input_buf_min_size_tiles: [0, 832, 0], ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [vstack: 7],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 4, u_kt: 1}}
    conv2d_654.dc.matmul.10: {type: matmul, grid_loc: [5, 1], grid_size: [2, 2], inputs: [conv2d_654.dc.sparse_matmul.9.lc2, layer4.0.downsample.0.weight, input_1_add_655_fork_clone87],
         t: 1, mblock: [1, 4], ublock: [1, 8], buf_size_mb: 2, input_buf_min_size_tiles: [48, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 2}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 0, u_kt: 4}}
    add_667: {type: add, grid_loc: [7, 4], grid_size: [2, 1], inputs: [conv2d_641.dc.matmul.8, conv2d_654.dc.matmul.10],
         t: 2, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 20], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 2], input_0_tms: [hslice: 2],
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_669.dc.matmul.8: {type: matmul, grid_loc: [7, 5], grid_size: [2, 2], inputs: [add_667, layer4.1.conv1.weight, input_1_add_670_fork_clone203],
         t: 1, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 2}], input_0_tms: [hstack: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 64, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 1}}
    conv2d_683.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [7, 7], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_683.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_669.dc.matmul.8, lc.input_tensor.conv2d_683.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 6, sparse_tile_ptr_bits: 5, u_kt: 2}}
    conv2d_683.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [9, 0], grid_size: [2, 1], inputs: [conv2d_683.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer4.1.conv2.weight], grid_transpose: true,
         t: 1, mblock: [1, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 12, min_buffer_input: 0, u_kt: 4}}
    conv2d_683.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [9, 2], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_683.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_669.dc.matmul.8, lc.input_tensor.conv2d_683.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1], grid_transpose: true,
         t: 1, mblock: [3, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 5, u_kt: 2}}
    conv2d_683.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [9, 4], grid_size: [2, 1], inputs: [conv2d_683.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer4.1.conv2.weight_fork_clone1859], grid_transpose: true,
         t: 1, mblock: [1, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 12, min_buffer_input: 0, u_kt: 4}}

  fwd_0_5_temporal_epoch_5:
    target_device: 0
    input_count: 64
    conv2d_683.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.conv2d_683.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, e2e_conv2d_669.dc.matmul.8_0, lc.input_tensor.conv2d_683.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [6, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 10, sparse_tile_ptr_bits: 5, u_kt: 2}}
    conv2d_683.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [1, 4], inputs: [conv2d_683.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer4.1.conv2.weight_fork_clone1861],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 8, min_buffer_input: 0, u_kt: 6}}
    _fused_op_15: {type: fused_op, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_conv2d_683.dc.conv2d.1.dc.matmul.11_0, e2e_conv2d_683.dc.conv2d.3.dc.matmul.11_0, conv2d_683.dc.conv2d.5.dc.matmul.11, input_1_add_684, input_1_add_684_fork_clone116],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {fused_op_id: 15}}
    conv2d_697.dc.matmul.8: {type: matmul, grid_loc: [1, 0], grid_size: [1, 8], inputs: [_fused_op_15, layer4.1.conv3.weight, input_1_add_698_fork_clone47],
         t: 1, mblock: [1, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 2}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 8, min_buffer_input: 0, u_kt: 2}}
    add_710: {type: add, grid_loc: [0, 6], grid_size: [1, 1], inputs: [conv2d_697.dc.matmul.8, e2e_add_667_0],
         t: 8, mblock: [1, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 4], input_0_tms: [hslice: 8],
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    conv2d_712.dc.matmul.8: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [add_710, layer4.2.conv1.weight, input_1_add_713_fork_clone151],
         t: 1, mblock: [1, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 2}], input_0_tms: [hstack: 8],
         attributes: {bias: true, kernel_broadcast: {input_2: 4}, m_k: 16, min_buffer_input: 0, relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00, u_kt: 4}}
    conv2d_726.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 0], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_726.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_712.dc.matmul.8, lc.input_tensor.conv2d_726.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 6, sparse_tile_ptr_bits: 5, u_kt: 1}}
    conv2d_726.dc.conv2d.5.dc.matmul.11: {type: matmul, grid_loc: [3, 1], grid_size: [1, 4], inputs: [conv2d_726.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer4.2.conv2.weight_fork_clone1895],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 8, min_buffer_input: 0, u_kt: 6}}
    conv2d_726.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 5], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_726.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_712.dc.matmul.8, lc.input_tensor.conv2d_726.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1], grid_transpose: true,
         t: 1, mblock: [3, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 6, sparse_tile_ptr_bits: 5, u_kt: 1}}
    conv2d_726.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [4, 1], grid_size: [1, 4], inputs: [conv2d_726.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer4.2.conv2.weight],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 8, min_buffer_input: 0, u_kt: 6}}
    conv2d_726.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [3, 7], grid_size: [2, 1], inputs: [lc.input_tensor.conv2d_726.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, conv2d_712.dc.matmul.8, lc.input_tensor.conv2d_726.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 1, mblock: [3, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Bfp8_b, RawUInt32], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 5, u_kt: 1}}
    conv2d_726.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [5, 0], grid_size: [1, 4], inputs: [conv2d_726.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, layer4.2.conv2.weight_fork_clone1893],
         t: 1, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [vslice: 3, hstack: 3],
         attributes: {m_k: 8, min_buffer_input: 0, u_kt: 6}}
    _fused_op_16: {type: fused_op, grid_loc: [4, 6], grid_size: [1, 1], inputs: [conv2d_726.dc.conv2d.1.dc.matmul.11, conv2d_726.dc.conv2d.3.dc.matmul.11, conv2d_726.dc.conv2d.5.dc.matmul.11, input_1_add_727, input_1_add_727_fork_clone70],
         t: 1, mblock: [1, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {fused_op_id: 15}}
    conv2d_740.dc.matmul.8: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [_fused_op_16, layer4.2.conv3.weight, input_1_add_741_fork_clone24],
         t: 1, mblock: [1, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 2}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 8, min_buffer_input: 0, u_kt: 2}}
    buffer_0_add_710_buffer_0_add_710_add_753: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_710],
         t: 8, mblock: [1, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [880], ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_710_add_753: {type: nop, grid_loc: [4, 5], grid_size: [1, 1], inputs: [buffer_0_add_710_buffer_0_add_710_add_753],
         t: 8, mblock: [1, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [880], ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_753: {type: add, grid_loc: [5, 4], grid_size: [1, 1], inputs: [conv2d_740.dc.matmul.8, buffer_0_add_710_add_753],
         t: 8, mblock: [1, 2], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 624], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hslice: 8],
         attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}}
    avg_pool2d_755.dc.reduce_avg.2.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [lc.input_tensor.avg_pool2d_755.dc.reduce_avg.2.0, add_753],
         t: 8, mblock: [1, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [broadcast: {r: 8}, vslice: 8],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}

  fwd_0_6_temporal_epoch_6:
    target_device: 0
    input_count: 64
    matmul_758: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [e2e_avg_pool2d_755.dc.reduce_avg.2.lc1_0, fc.weight],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 8],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 64}}
    add_759: {type: add, grid_loc: [1, 0], grid_size: [1, 1], inputs: [matmul_758, fc.bias], untilize_output: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}


programs:
  - run_fwd_0:
    - var: {$c_microbatch_size: 64, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q0: 0, $lptr_q6: 0, $gptr_q1: 0, $gptr_q6: 0, $lptr_q5: 0, $gptr_q5: 0, $lptr_q2: 0, $lptr_q4: 0, $lptr_q3: 0, $lptr_q1: 0, $lptr_q0: 0, $gptr_q4: 0, $gptr_q3: 0, $gptr_q2: 0}
    - loop: 1
    -   execute: {graph_name: fwd_0_0_temporal_epoch_0, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               conv1.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               conv1.weight_fork_clone1919: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               conv1.weight_fork_clone1921: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               conv1.weight_fork_clone1923: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [insert_queue0, insert_queue1, buf_max_pool2d_14.dc.reduce_max.6_0, buf_add_69_0]
    -   execute: {graph_name: fwd_0_1_temporal_epoch_1, queue_settings: {
               insert_queue0: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               insert_queue1: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               e2e_conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_conv2d_0.dc.conv2d.3.dc.conv2d.5.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_conv2d_0.dc.conv2d.3.dc.conv2d.7.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               buf_max_pool2d_14.dc.reduce_max.6_0: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               buf_add_69_0: {prologue: false, epilogue: false, zero: False, global_rdptr_autoinc: 1, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_1_fork_clone1120: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.max_pool2d_14.dc.sparse_matmul.5.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.max_pool2d_14.dc.sparse_matmul.5.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer1.0.conv1.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_16_fork_clone1163: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_29.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer1.0.conv2.weight_fork_clone1327: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_29.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer1.0.conv2.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_29.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer1.0.conv2.weight_fork_clone1325: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_30: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_30_fork_clone1095: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer1.0.conv3.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_44_fork_clone1005: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer1.0.downsample.0.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_57_fork_clone1010: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer1.1.conv1.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_72_fork_clone1130: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_85.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_85.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer1.1.conv2.weight_fork_clone1374: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_85.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_85.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer1.1.conv2.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_85.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_85.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer1.1.conv2.weight_fork_clone1372: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_86: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_86_fork_clone1047: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer1.1.conv3.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_100_fork_clone952: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer1.2.conv1.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_115_fork_clone1083: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_128.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_128.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer1.2.conv2.weight_fork_clone1408: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_128.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_128.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer1.2.conv2.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_128.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_128.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer1.2.conv2.weight_fork_clone1406: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_129: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_129_fork_clone993: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer1.2.conv3.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_143_fork_clone899: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.0.conv1.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_158_fork_clone965: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_171.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_171.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.0.conv2.weight_fork_clone1442: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_171.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_171.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.0.conv2.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_171.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_171.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.0.conv2.weight_fork_clone1440: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_172: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_172_fork_clone871: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.0.conv3.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_186_fork_clone769: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_198.dc.sparse_matmul.9.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_198.dc.sparse_matmul.9.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.0.downsample.0.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_199_fork_clone774: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.1.conv1.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_214_fork_clone913: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_227.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_227.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.1.conv2.weight_fork_clone1491: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_227.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_227.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.1.conv2.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_227.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_227.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.1.conv2.weight_fork_clone1489: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_228: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_228_fork_clone815: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.1.conv3.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_242_fork_clone714: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [insert_queue0, insert_queue1, buf_max_pool2d_14.dc.reduce_max.6_0, buf_add_69_0]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_0_2_temporal_epoch_2, queue_settings: {
               e2e_conv2d_241.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_buffer_0_add_211_add_254_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               layer2.2.conv1.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_257_fork_clone859: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_270.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_270.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.2.conv2.weight_fork_clone1525: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_270.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.2.conv2.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_270.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_270.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.2.conv2.weight_fork_clone1523: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_271: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_271_fork_clone757: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.2.conv3.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_285_fork_clone663: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.3.conv1.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_300_fork_clone803: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_313.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_313.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.3.conv2.weight_fork_clone1559: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_313.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_313.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.3.conv2.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_313.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_313.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.3.conv2.weight_fork_clone1557: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_314: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_314_fork_clone702: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer2.3.conv3.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_328_fork_clone610: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.0.conv1.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_343_fork_clone675: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_356.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_356.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.0.conv2.weight_fork_clone1593: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_356.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_356.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.0.conv2.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_356.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_356.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.0.conv2.weight_fork_clone1591: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_357: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_357_fork_clone580: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.0.conv3.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_371_fork_clone466: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_383.dc.sparse_matmul.9.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_383.dc.sparse_matmul.9.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.0.downsample.0.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_384_fork_clone471: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.1.conv1.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_399_fork_clone624: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_412.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_412.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.1.conv2.weight_fork_clone1642: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_412.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_412.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.1.conv2.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_412.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_412.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.1.conv2.weight_fork_clone1640: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_413: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_413_fork_clone520: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.1.conv3.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_427_fork_clone401: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.2.conv1.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_442_fork_clone568: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_455.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_455.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.2.conv2.weight_fork_clone1676: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_455.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_455.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.2.conv2.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_0_3_temporal_epoch_3, queue_settings: {
               e2e_conv2d_455.dc.conv2d.5.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_conv2d_455.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_conv2d_455.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_buffer_0_add_439_buffer_0_add_439_buffer_0_add_439_add_482_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               layer3.2.conv2.weight_fork_clone1674: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_456: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_456_fork_clone454: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.2.conv3.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_470_fork_clone344: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.3.conv1.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_485_fork_clone508: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_498.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_498.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.3.conv2.weight_fork_clone1710: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_0_4_temporal_epoch_4, queue_settings: {
               e2e_conv2d_484.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_conv2d_498.dc.conv2d.5.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_buffer_0_add_482_buffer_0_add_482_buffer_0_add_482_buffer_0_add_482_add_525_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               lc.input_tensor.conv2d_498.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_498.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.3.conv2.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_498.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_498.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.3.conv2.weight_fork_clone1708: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_499: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_499_fork_clone389: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.3.conv3.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_513_fork_clone291: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.4.conv1.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_528_fork_clone442: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_541.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_541.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.4.conv2.weight_fork_clone1744: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_541.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_541.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.4.conv2.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_541.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_541.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.4.conv2.weight_fork_clone1742: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_542: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_542_fork_clone332: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.4.conv3.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_556_fork_clone240: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.5.conv1.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_571_fork_clone377: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_584.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_584.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.5.conv2.weight_fork_clone1778: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_584.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_584.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.5.conv2.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_584.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_584.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.5.conv2.weight_fork_clone1776: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_585: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_585_fork_clone279: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer3.5.conv3.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_599_fork_clone190: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer4.0.conv1.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_614_fork_clone252: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_627.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_627.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer4.0.conv2.weight_fork_clone1812: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_627.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_627.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer4.0.conv2.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_627.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_627.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer4.0.conv2.weight_fork_clone1810: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_628: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_628_fork_clone163: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer4.0.conv3.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_642_fork_clone82: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_654.dc.sparse_matmul.9.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_654.dc.sparse_matmul.9.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer4.0.downsample.0.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_655_fork_clone87: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer4.1.conv1.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_670_fork_clone203: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_683.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_683.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer4.1.conv2.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_683.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_683.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer4.1.conv2.weight_fork_clone1859: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_0_5_temporal_epoch_5, queue_settings: {
               e2e_add_667_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_conv2d_669.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_conv2d_683.dc.conv2d.1.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_conv2d_683.dc.conv2d.3.dc.matmul.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               lc.input_tensor.conv2d_683.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_683.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer4.1.conv2.weight_fork_clone1861: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_684: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_684_fork_clone116: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer4.1.conv3.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_698_fork_clone47: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer4.2.conv1.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_713_fork_clone151: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_726.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_726.dc.conv2d.5.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer4.2.conv2.weight_fork_clone1895: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_726.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_726.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer4.2.conv2.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_726.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_726.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer4.2.conv2.weight_fork_clone1893: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_727: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_727_fork_clone70: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer4.2.conv3.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_741_fork_clone24: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.avg_pool2d_755.dc.reduce_avg.2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 128]
    -   execute: {graph_name: fwd_0_6_temporal_epoch_6, queue_settings: {
               e2e_avg_pool2d_755.dc.reduce_avg.2.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               fc.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               fc.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 128]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 128]
    - endloop


fused_ops:
  0: 
    inputs: 6
    intermediates: 1
    schedules: 
      -
        - conv2d_0.dc.conv2d.3.dc.add.8: { type: add, inputs: [input0, input1], mblock: [98, 1], ublock: [2, 2], output: intermed0}
        - conv2d_0.dc.conv2d.3.dc.add.9: { type: add, inputs: [input2, input3], mblock: [98, 1], ublock: [2, 2], output: dest}
        - conv2d_0.dc.conv2d.3.dc.add.10: { type: add, inputs: [intermed0, dest], pop: [intermed0], mblock: [98, 1], ublock: [2, 2], output: dest}
        - multiply_7: { type: multiply, inputs: [dest, input4], mblock: [98, 1], ublock: [2, 2], output: dest}
        - add_12: { type: add, inputs: [dest, input5], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [98, 1], ublock: [2, 2], output: output}
  1: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_29.dc.add.6: { type: add, inputs: [input0, input1], mblock: [49, 1], ublock: [2, 2], output: dest}
        - conv2d_29.dc.add.7: { type: add, inputs: [input2, dest], mblock: [49, 1], ublock: [2, 2], output: dest}
        - multiply_36: { type: multiply, inputs: [dest, input3], mblock: [49, 1], ublock: [2, 2], output: dest}
        - add_41: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [49, 1], ublock: [2, 2], output: output}
  2: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_85.dc.add.6: { type: add, inputs: [input0, input1], mblock: [7, 1], ublock: [2, 2], output: dest}
        - conv2d_85.dc.add.7: { type: add, inputs: [input2, dest], mblock: [7, 1], ublock: [2, 2], output: dest}
        - multiply_92: { type: multiply, inputs: [dest, input3], mblock: [7, 1], ublock: [2, 2], output: dest}
        - add_97: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [7, 1], ublock: [2, 2], output: output}
  4: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_171.dc.add.6: { type: add, inputs: [input0, input1], mblock: [1, 1], ublock: [1, 4], output: dest}
        - conv2d_171.dc.add.7: { type: add, inputs: [input2, dest], mblock: [1, 1], ublock: [1, 4], output: dest}
        - multiply_178: { type: multiply, inputs: [dest, input3], mblock: [1, 1], ublock: [1, 4], output: dest}
        - add_183: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [1, 1], ublock: [1, 4], output: output}
  5: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_227.dc.add.6: { type: add, inputs: [input0, input1], mblock: [25, 1], ublock: [1, 4], output: dest}
        - conv2d_227.dc.add.7: { type: add, inputs: [input2, dest], mblock: [25, 1], ublock: [1, 4], output: dest}
        - multiply_234: { type: multiply, inputs: [dest, input3], mblock: [25, 1], ublock: [1, 4], output: dest}
        - add_239: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [25, 1], ublock: [1, 4], output: output}
  8: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_356.dc.add.6: { type: add, inputs: [input0, input1], mblock: [7, 2], ublock: [1, 4], output: dest}
        - conv2d_356.dc.add.7: { type: add, inputs: [input2, dest], mblock: [7, 2], ublock: [1, 4], output: dest}
        - multiply_363: { type: multiply, inputs: [dest, input3], mblock: [7, 2], ublock: [1, 4], output: dest}
        - add_368: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [7, 2], ublock: [1, 4], output: output}
  10: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_455.dc.add.6: { type: add, inputs: [input0, input1], mblock: [1, 2], ublock: [1, 4], output: dest}
        - conv2d_455.dc.add.7: { type: add, inputs: [input2, dest], mblock: [1, 2], ublock: [1, 4], output: dest}
        - multiply_462: { type: multiply, inputs: [dest, input3], mblock: [1, 2], ublock: [1, 4], output: dest}
        - add_467: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [1, 2], ublock: [1, 4], output: output}
  14: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_627.dc.add.6: { type: add, inputs: [input0, input1], mblock: [1, 4], ublock: [1, 4], output: dest}
        - conv2d_627.dc.add.7: { type: add, inputs: [input2, dest], mblock: [1, 4], ublock: [1, 4], output: dest}
        - multiply_634: { type: multiply, inputs: [dest, input3], mblock: [1, 4], ublock: [1, 4], output: dest}
        - add_639: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [1, 4], ublock: [1, 4], output: output}
  15: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_683.dc.add.6: { type: add, inputs: [input0, input1], mblock: [1, 4], ublock: [2, 4], output: dest}
        - conv2d_683.dc.add.7: { type: add, inputs: [input2, dest], mblock: [1, 4], ublock: [2, 4], output: dest}
        - multiply_690: { type: multiply, inputs: [dest, input3], mblock: [1, 4], ublock: [2, 4], output: dest}
        - add_695: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: min, relu_threshold: 0.000000e+00}, mblock: [1, 4], ublock: [2, 4], output: output}


test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pcc: 0.99
    check_pct: 0.90
    verbosity: Concise
  stimulus-config:
    type: Uniform
    uniform_lower_bound: -1.0
    uniform_upper_bound: 1.0
